"""
–ê–Ω–∞–ª–∏–∑ feature importance –∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è ML –º–æ–¥–µ–ª–µ–π.
"""
import sys
import os
import pickle
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple
import json
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
sys.path.insert(0, str(Path(__file__).parent))

def load_model_metadata(model_path: str) -> Dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏."""
    try:
        with open(model_path, 'rb') as f:
            data = pickle.load(f)
            return data
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {model_path}: {e}")
        return None

def extract_feature_importance(model_data: Dict, model_name: str) -> pd.DataFrame:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç feature importance –∏–∑ –º–æ–¥–µ–ª–∏."""
    model = model_data.get('model')
    feature_names = model_data.get('feature_names', [])
    
    if not feature_names:
        print(f"‚ö†Ô∏è  –ù–µ—Ç feature_names –≤ {model_name}")
        return pd.DataFrame()
    
    importance_dict = {}
    
    # –î–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
    if hasattr(model, 'feature_importances_'):
        # Random Forest, XGBoost, LightGBM
        importances = model.feature_importances_
        for i, feature in enumerate(feature_names):
            if i < len(importances):
                importance_dict[feature] = importances[i]
    
    elif hasattr(model, 'rf_model') and hasattr(model, 'xgb_model'):
        # Ensemble: –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ importance –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        rf_imp = model.rf_model.feature_importances_ if hasattr(model.rf_model, 'feature_importances_') else None
        xgb_imp = model.xgb_model.feature_importances_ if hasattr(model.xgb_model, 'feature_importances_') else None
        
        if rf_imp is not None and xgb_imp is not None:
            avg_imp = (rf_imp + xgb_imp) / 2
            for i, feature in enumerate(feature_names):
                if i < len(avg_imp):
                    importance_dict[feature] = avg_imp[i]
        elif rf_imp is not None:
            for i, feature in enumerate(feature_names):
                if i < len(rf_imp):
                    importance_dict[feature] = rf_imp[i]
        elif xgb_imp is not None:
            for i, feature in enumerate(feature_names):
                if i < len(xgb_imp):
                    importance_dict[feature] = xgb_imp[i]
    
    elif hasattr(model, 'rf_model') and hasattr(model, 'xgb_model') and hasattr(model, 'lgb_model'):
        # TripleEnsemble: –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç –≤—Å–µ—Ö —Ç—Ä–µ—Ö
        rf_imp = model.rf_model.feature_importances_ if hasattr(model.rf_model, 'feature_importances_') else None
        xgb_imp = model.xgb_model.feature_importances_ if hasattr(model.xgb_model, 'feature_importances_') else None
        lgb_imp = model.lgb_model.feature_importances_ if hasattr(model.lgb_model, 'feature_importances_') else None
        
        imps = [imp for imp in [rf_imp, xgb_imp, lgb_imp] if imp is not None]
        if imps:
            avg_imp = np.mean(imps, axis=0)
            for i, feature in enumerate(feature_names):
                if i < len(avg_imp):
                    importance_dict[feature] = avg_imp[i]
    
    elif hasattr(model, 'metrics') and 'feature_importance' in model.metrics:
        # –ò–∑ –º–µ—Ç—Ä–∏–∫
        importance_dict = model.metrics['feature_importance']
    
    if not importance_dict:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å importance –∏–∑ {model_name}")
        return pd.DataFrame()
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame([
        {'feature': feat, 'importance': imp}
        for feat, imp in importance_dict.items()
    ])
    df = df.sort_values('importance', ascending=False)
    df['model'] = model_name
    
    return df

def analyze_correlations(feature_names: List[str], data_path: str = None) -> pd.DataFrame:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É —Ñ–∏—á–∞–º–∏."""
    # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏—Ö
    if data_path and os.path.exists(data_path):
        try:
            df = pd.read_csv(data_path)
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Ñ–∏—á–∏
            feature_cols = [col for col in feature_names if col in df.columns]
            if feature_cols:
                corr_matrix = df[feature_cols].corr()
                return corr_matrix
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")
    
    return pd.DataFrame()

def main():
    print("=" * 80)
    print("üìä –ê–ù–ê–õ–ò–ó FEATURE IMPORTANCE –ò –ö–û–†–†–ï–õ–Ø–¶–ò–ô")
    print("=" * 80)
    
    # –ò—â–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –≤ ml_models/
    models_dir = Path("ml_models")
    if not models_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {models_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ MTF –º–æ–¥–µ–ª–∏ BTCUSDT (–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
    model_files = list(models_dir.glob("*BTCUSDT*mtf*.pkl"))
    
    if not model_files:
        print(f"‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ MTF –º–æ–¥–µ–ª–µ–π –¥–ª—è BTCUSDT")
        # –ü—Ä–æ–±—É–µ–º –ª—é–±—ã–µ –º–æ–¥–µ–ª–∏
        model_files = list(models_dir.glob("*.pkl"))[:5]
    
    print(f"\nüì¶ –ù–∞–π–¥–µ–Ω–æ {len(model_files)} –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    all_importances = []
    
    for model_file in model_files:
        model_name = model_file.stem
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑: {model_name}")
        
        model_data = load_model_metadata(str(model_file))
        if model_data is None:
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º feature importance
        importance_df = extract_feature_importance(model_data, model_name)
        if not importance_df.empty:
            all_importances.append(importance_df)
            print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(importance_df)} —Ñ–∏—á–µ–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10
            top10 = importance_df.head(10)
            print(f"\n   üìà –¢–û–ü-10 –≤–∞–∂–Ω—ã—Ö —Ñ–∏—á–µ–π:")
            for idx, row in top10.iterrows():
                print(f"      {row['feature']:<30} {row['importance']:>8.4f}")
        else:
            print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å importance")
    
    if not all_importances:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å importance –Ω–∏ –∏–∑ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏")
        return
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ importance
    combined_df = pd.concat(all_importances, ignore_index=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∏—á–∞–º –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ importance
    feature_avg = combined_df.groupby('feature')['importance'].agg(['mean', 'std', 'count']).reset_index()
    feature_avg = feature_avg.sort_values('mean', ascending=False)
    
    print("\n" + "=" * 80)
    print("üìä –°–†–ï–î–ù–Ø–Ø IMPORTANCE –ü–û –í–°–ï–ú –ú–û–î–ï–õ–Ø–ú")
    print("=" * 80)
    print(f"\n{'–§–∏—á–∞':<40} | {'–°—Ä–µ–¥–Ω–µ–µ':<10} | {'Std':<10} | {'–ú–æ–¥–µ–ª–µ–π':<8}")
    print("-" * 75)
    
    for idx, row in feature_avg.head(30).iterrows():
        print(f"{row['feature']:<40} | {row['mean']:>9.4f} | {row['std']:>9.4f} | {int(row['count']):>7}")
    
    # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ñ–∏—á–µ–π
    print("\n" + "=" * 80)
    print("üìä –ê–ù–ê–õ–ò–ó –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú –§–ò–ß–ï–ô")
    print("=" * 80)
    
    categories = {
        'MTF': [f for f in feature_avg['feature'] if any(x in f for x in ['_60', '_240', 'rsi_60', 'rsi_240'])],
        '–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å': [f for f in feature_avg['feature'] if any(x in f for x in ['volatility', 'atr', 'parkinson'])],
        '–ü–∞—Ç—Ç–µ—Ä–Ω—ã': [f for f in feature_avg['feature'] if any(x in f for x in ['is_', 'doji', 'hammer', 'engulfing'])],
        'S/R': [f for f in feature_avg['feature'] if any(x in f for x in ['support', 'resistance', 'local_'])],
        '–¢—Ä–µ–Ω–¥': [f for f in feature_avg['feature'] if any(x in f for x in ['ema', 'sma', 'adx', 'di_', 'trend'])],
        'RSI': [f for f in feature_avg['feature'] if 'rsi' in f],
        '–û–±—ä–µ–º': [f for f in feature_avg['feature'] if 'volume' in f],
        '–ú–∏–∫—Ä–æ—Å—Ç—Ä—É–∫—Ç—É—Ä–∞': [f for f in feature_avg['feature'] if any(x in f for x in ['spread', 'imbalance', 'momentum'])],
    }
    
    for category, features in categories.items():
        if features:
            cat_importance = feature_avg[feature_avg['feature'].isin(features)]['mean'].mean()
            print(f"\n{category:<20}: {len(features):>3} —Ñ–∏—á–µ–π, —Å—Ä–µ–¥–Ω—è—è importance: {cat_importance:.4f}")
            top_cat = feature_avg[feature_avg['feature'].isin(features)].head(5)
            for idx, row in top_cat.iterrows():
                print(f"   {row['feature']:<35} {row['mean']:>8.4f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_dir = Path("backtest_reports")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"feature_importance_analysis_{timestamp}.csv"
    
    feature_avg.to_csv(output_file, index=False)
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ JSON
    json_file = output_dir / f"feature_importance_analysis_{timestamp}.json"
    report = {
        'timestamp': timestamp,
        'models_analyzed': len(model_files),
        'total_features': len(feature_avg),
        'top_features': feature_avg.head(30).to_dict('records'),
        'categories': {
            cat: {
                'count': len(features),
                'avg_importance': float(feature_avg[feature_avg['feature'].isin(features)]['mean'].mean()) if features else 0.0,
                'top_features': feature_avg[feature_avg['feature'].isin(features)].head(5).to_dict('records') if features else []
            }
            for cat, features in categories.items()
        }
    }
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {json_file}")
    print("\n" + "=" * 80)
    print("‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 80)

if __name__ == "__main__":
    main()
