"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LightGBM –º–æ–¥–µ–ª–∏ (—Ç—Ä–µ—Ç—å—è ML —Å—Ç—Ä–∞—Ç–µ–≥–∏—è).
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python train_lightgbm_model.py --symbol BTCUSDT --days 180
"""
import warnings
import os
import argparse
import sys
from pathlib import Path

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
os.environ['PYTHONWARNINGS'] = 'ignore::UserWarning'
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

from bot.config import load_settings
from bot.ml.data_collector import DataCollector
from bot.ml.feature_engineering import FeatureEngineer
from bot.ml.model_trainer import ModelTrainer, LIGHTGBM_AVAILABLE


def main():
    parser = argparse.ArgumentParser(description='Train LightGBM ML model')
    parser.add_argument('--symbol', type=str, default='BTCUSDT', 
                       help='Trading symbol (default: BTCUSDT)')
    parser.add_argument('--days', type=int, default=180,
                       help='Number of days of historical data (default: 180)')
    parser.add_argument('--interval', type=str, default='15m',
                       help='Timeframe interval (default: 15m)')
    parser.add_argument('--ensemble', action='store_true',
                       help='Train triple ensemble (RF+XGB+LGB) instead of single LightGBM')
    parser.add_argument('--n_estimators', type=int, default=150,
                       help='Number of estimators for LightGBM (default: 150)')
    parser.add_argument('--max_depth', type=int, default=7,
                       help='Max depth for LightGBM (default: 7)')
    parser.add_argument('--learning_rate', type=float, default=0.05,
                       help='Learning rate for LightGBM (default: 0.05)')
    
    args = parser.parse_args()
    
    if not LIGHTGBM_AVAILABLE:
        print("‚ùå ERROR: LightGBM is not installed!")
        print("   Install with: pip install lightgbm>=4.0.0")
        return
    
    print("=" * 70)
    print("üöÄ LightGBM ML Model Training")
    print("=" * 70)
    print(f"Symbol: {args.symbol}")
    print(f"Days: {args.days}")
    print(f"Interval: {args.interval}")
    print(f"Mode: {'Triple Ensemble (RF+XGB+LGB)' if args.ensemble else 'Single LightGBM'}")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    settings = load_settings()
    
    # === –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö ===
    print(f"\n[Step 1] Collecting historical data for {args.symbol}...")
    collector = DataCollector(settings.api)
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df_raw = collector.collect_klines(
        symbol=args.symbol,
        interval=args.interval.replace('m', ''),
        start_date=None,
        end_date=None,
        limit=200,
    )
    
    if df_raw.empty:
        print(f"‚ùå No data collected for {args.symbol}. Skipping.")
        return
    
    print(f"‚úÖ Collected {len(df_raw)} candles")
    
    # === –®–∞–≥ 2: Feature Engineering ===
    print(f"\n[Step 2] Creating features...")
    feature_engineer = FeatureEngineer()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    df_features = feature_engineer.create_technical_indicators(df_raw)
    print(f"‚úÖ Created {len(feature_engineer.get_feature_names())} features")
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    print(f"\n[Step 3] Creating target variable...")
    df_with_target = feature_engineer.create_target_variable(
        df_features,
        forward_periods=5,  # 5 * 15m = 75 –º–∏–Ω—É—Ç
        threshold_pct=1.0,  # 1.0% –ø–æ—Ä–æ–≥
        use_atr_threshold=True,
        use_risk_adjusted=True,
        min_risk_reward_ratio=2.0,  # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ä–∏—Å–∫/–ø—Ä–∏–±—ã–ª—å 2:1 (—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ—Ä–≥–æ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º TP=25%, SL=10%)
        max_hold_periods=48,  # –ú–∞–∫—Å–∏–º—É–º 48 * 15m = 12 —á–∞—Å–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–¥–µ–ª–æ–∫ (—Å–º—è–≥—á–µ–Ω–æ: –±—ã–ª–æ 32)
        min_profit_pct=1.0,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å 1.0% –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞–∫ LONG/SHORT (—Å–º—è–≥—á–µ–Ω–æ: –±—ã–ª–æ 1.5%)
    )
    
    target_dist = df_with_target['target'].value_counts().to_dict()
    print(f"‚úÖ Target distribution:")
    for target_val, count in sorted(target_dist.items()):
        pct = (count / len(df_with_target)) * 100
        target_name = {-1: "SHORT", 0: "HOLD", 1: "LONG"}.get(
            target_val, f"UNKNOWN({target_val})")
        print(f"    {target_name:6s}: {count:5d} ({pct:5.1f}%)")
    
    # === –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML ===
    print(f"\n[Step 4] Preparing data for ML...")
    X, y = feature_engineer.prepare_features_for_ml(df_with_target)
    print(f"‚úÖ Prepared data: X.shape={X.shape}, y.shape={y.shape}")
    
    # === –®–∞–≥ 5: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print(f"\n[Step 5] Training model...")
    trainer = ModelTrainer()
    
    if args.ensemble:
        # –û–±—É—á–∞–µ–º —Ç—Ä–æ–π–Ω–æ–π –∞–Ω—Å–∞–º–±–ª—å
        print(f"\nüéØ Training Triple Ensemble (RF + XGBoost + LightGBM)...")
        model, metrics = trainer.train_ensemble(
            X, y,
            ensemble_method="triple",
            include_lightgbm=True,
            rf_n_estimators=100,
            rf_max_depth=10,
            xgb_n_estimators=100,
            xgb_max_depth=6,
            xgb_learning_rate=0.1,
            lgb_n_estimators=args.n_estimators,
            lgb_max_depth=args.max_depth,
            lgb_learning_rate=args.learning_rate,
        )
        
        model_filename = f"triple_ensemble_{args.symbol}_{args.interval.replace('m', '')}.pkl"
        model_type = "triple_ensemble"
        
        print(f"\nüìä Triple Ensemble Results:")
        print(f"  CV Accuracy: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std'] * 2:.4f})")
        print(f"  F1-Score: {metrics.get('cv_f1_mean', 0):.4f}")
        print(f"  Weights: RF={metrics['rf_weight']:.3f}, "
              f"XGB={metrics['xgb_weight']:.3f}, "
              f"LGB={metrics['lgb_weight']:.3f}")
    else:
        # –û–±—É—á–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é LightGBM –º–æ–¥–µ–ª—å
        print(f"\nüéØ Training LightGBM Classifier...")
        model, metrics = trainer.train_lightgbm_classifier(
            X, y,
            n_estimators=args.n_estimators,
            max_depth=args.max_depth,
            learning_rate=args.learning_rate,
        )
        
        model_filename = f"lgb_{args.symbol}_{args.interval.replace('m', '')}.pkl"
        model_type = "lightgbm"
        
        print(f"\nüìä LightGBM Results:")
        print(f"  Accuracy: {metrics['accuracy']:.4f}")
        print(f"  CV Accuracy: {metrics['cv_mean']:.4f} (+/- {metrics['cv_std'] * 2:.4f})")
    
    # === –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
    print(f"\n[Step 6] Saving model...")
    trainer.save_model(
        model,
        trainer.scaler,
        feature_engineer.get_feature_names(),
        metrics,
        model_filename,
        symbol=args.symbol,
        interval=args.interval.replace('m', ''),
        model_type=model_type,
    )
    
    print(f"‚úÖ Model saved: {model_filename}")
    print(f"\nüéâ Training completed successfully!")
    print(f"\nüí° Next steps:")
    print(f"   1. Test the model: python -m bot.ml.diagnose_model {model_filename}")
    print(f"   2. Use in live trading: Enable ML strategy in config")
    print(f"   3. Compare with other models: Check backtest results")


if __name__ == "__main__":
    main()
